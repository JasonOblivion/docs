---
title: Generating Completions
description: "You can quickly start generating completions through OpenAI or locally with the LLM-VM in 3 lines of code. Just specify a ```big_model``` you can select where your completions are generated from."
---

## Examples
Here we give two examples of how you can generate completions with our LLM-VM.
- ```OpenAI Endpoint``` call's OpenAI's [gpt-3.5-turbo](https://platform.openai.com/docs/models/gpt-3-5) model for a completion, which requires your OpenAI API Key and utilize's their endpoint.
- ```Local Endpoint``` example shows you how you can locally use an LLM to generate completions just as easily.

<CodeGroup>
```python OpenAI Endpoint
# import our client
from llm_vm.client import Client

# Selecting the Chat GPT endpoint from OpenAI 
client=Client(big_model='chat_gpt')

# Put in your prompt and go!
response=client.complete(
	prompt='What is Anarchy?',
	context='',
	openai_key='OPENAI_API_KEY')
print(response)
# Anarchy is a political ideology that advocates for the absence of government...
```

```python Local Endpoint
# import our client
from llm_vm.client import Client

# Selecting the Bloom LLM for generation
client=Client(big_model='bloom')

# Put in your prompt and go!
response=client.complete(
	prompt='What is Anarchy?',
	context='') # OPENAI_API_KEY only needed for OpenAI calls
print(response)
# Anarchy is a political ideology that advocates for the absence of government...
```

</CodeGroup>

<Warning>Using OpenAI's models require an OpenAI API Key and may result in costs not associated with Anarchy's LLM-VM</Warning>

## Supported Models
You can see a list of all models you can use below.

<Accordion title="Supported Models (and sizes)">
|```big_model```|Developer|LLM model|Model Size|
|-|-|-|-|
|```'chat_gpt'```|[OpenAI](https://platform.openai.com/)			 | [gpt-3.5-turbo-0301](https://platform.openai.com/docs/models/gpt-3-5)     |n/a|
|```'gpt'```     |[OpenAI](https://platform.openai.com/)			 | [text-davinci-003](https://platform.openai.com/docs/models/gpt-3)         |n/a|
|```'bloom'```   |[BigScience](https://huggingface.co/bigscience)         | [bloom-560](https://huggingface.co/bigscience/bloom-560m)                 |1.12 GB|
|```'flan'```    |[Google](https://huggingface.co/google)	                 | [flan-t5-small](https://huggingface.co/google/flan-t5-small)              |308 MB|
|```'llama'```   |[Openlm-Research](https://huggingface.co/openlm-research)| [open_llama_3b](https://huggingface.co/openlm-research/open_llama_3b)     |6.85 GB|
|```'neo'```     |[ElutherAI](https://huggingface.co/EleutherAI)      	 | [gpt-neo-1.3B](https://huggingface.co/EleutherAI/gpt-neo-1.3B)	     |5.31 GB|
|```'opt'```     |[Facebook](https://huggingface.co/facebook)       	 | [opt-350m](https://huggingface.co/facebook/opt-350m)                      |662 MB|
|```'pythia'```  |[ElutherAI](https://huggingface.co/EleutherAI)      	 | [pythia-70m-deduped](https://huggingface.co/EleutherAI/pythia-70m-deduped)|166 MB|

Just replace ```'chat_gpt'``` in the example with your desired model and you're all set.

</Accordion>

<Card
    title="Visit our Github Repo"
    icon="pen-to-square"
    href="https://github.com/anarchy-ai/llm-vm"
    >
    Interested in learning more? Come see the code!
</Card> 